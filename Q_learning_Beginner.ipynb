{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Il3waFud1ABY"
      ],
      "authorship_tag": "ABX9TyPtz4ESyVr4PoYeyhMLPt2G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZAHID188/Q-learning_collab/blob/main/Q_learning_Beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning lesson\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFIBR75pfCdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Q-table: The Q-table is a matrix that represents the Q-values for each state-action pair. In this example, we initialize a Q-table with zeros."
      ],
      "metadata": {
        "id": "o3cTq3-MgE-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Q-table\n",
        "num_states = 5\n",
        "num_actions = 3\n",
        "Q = np.zeros((num_states, num_actions)) #make a row-5, column-3 array   Creating a 2-dimensional array\n",
        "print(Q)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwYsZieqfO-u",
        "outputId": "74dd91b9-8a2e-411b-e7f8-3472724b5c71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the rewards matrix: The rewards matrix defines the rewards for each state-action pair. In this example, we use a 5x3 matrix where -1 represents a negative reward and 100 represents a positive reward for reaching the goal state.\n",
        "\n",
        "The value 0 in the last row indicates that no reward is given for taking an action in the goal state."
      ],
      "metadata": {
        "id": "KLYgdtm8hI6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the rewards matrix\n",
        "rewards = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [0, -1, 100]])\n",
        "print(rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHY-i9z7ffln",
        "outputId": "8661a9ea-6448-4529-f421-45f49958cb03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [  0  -1 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the discount factor: The discount factor (gamma) determines the importance of future rewards compared to immediate rewards. In this example, we set gamma to 0.8.\n",
        "\n",
        "Define the learning rate: The learning rate determines the impact of new information on the Q-values. In this example, we set the learning rate to 0.5.\n",
        "\n",
        "\n",
        "Define the number of episodes: An episode represents a complete run-through of the Q-learning algorithm. In this example, we set the number of episodes to 1000."
      ],
      "metadata": {
        "id": "NJfEtx49hhhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discount factor\n",
        "gamma = 0.8\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Define the number of episodes\n",
        "num_episodes = 1000\n",
        "\n"
      ],
      "metadata": {
        "id": "IN1SSwc6fs7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Q-learning algorithm: The Q-learning algorithm consists of multiple episodes, where the agent learns to update the Q-values based on the rewards received.\n",
        "    \n",
        "    a. Initialize the state: At the start of each episode, we randomly select an initial state.\n",
        "    \n",
        "    ```python\n",
        "    state = np.random.randint(0, num_states)\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    b. Perform Q-learning until the goal state is reached:\n",
        "    \n",
        "    i. Choose an action: The agent selects an action based on the highest Q-value for the current state.\n",
        "    \n",
        "    ```python\n",
        "    action = np.argmax(Q[state])\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    ii. Transition to the next state: The agent transitions to the next state by randomly selecting one.\n",
        "    \n",
        "    ```python\n",
        "    next_state = np.random.randint(0, num_states)\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    iii. Get the reward: The agent receives a reward based on the current state-action pair.\n",
        "    \n",
        "    ```python\n",
        "    reward = rewards[state, action]\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    iv. Update the Q-value: The Q-value for the current state-action pair is updated using the Q-learning equation.\n",
        "    \n",
        "    ```python\n",
        "    Q[state, action] += learning_rate * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    v. Move to the next state: The agent moves to the next state.\n",
        "    \n",
        "    ```python\n",
        "    state = next_state\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    vi. Check if the goal state is reached: If the agent reaches the goal state, the episode ends.\n",
        "    \n",
        "    ```python\n",
        "    if state == num_states - 1:\n",
        "        done = True\n",
        "    \n",
        "    ```"
      ],
      "metadata": {
        "id": "-3zEqpGSiHJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## np.argmax\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "# Create a 1-dimensional array\n",
        "arr = np.array([2, 5, 1, 8, 4])\n",
        "\n",
        "# Find the index of the maximum value in the array\n",
        "max_index = np.argmax(arr)\n",
        "\n",
        "print(max_index)\n",
        "# Output: 3\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OQBCPG1RjC8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning algorithm"
      ],
      "metadata": {
        "id": "uM566aosjeKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(0, num_states)  # Initial state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(Q[state])  # Choose action with highest Q-value\n",
        "\n",
        "        next_state = np.random.randint(0, num_states)  # Randomly select next state\n",
        "        reward = rewards[state, action]  # Get reward for current state-action pair\n",
        "\n",
        "        Q[state, action] += learning_rate * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "        if state == num_states - 1:\n",
        "            done = True\n",
        "\n"
      ],
      "metadata": {
        "id": "pCI5hEgvfzE8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the learned policy\n",
        "state = 0  # Initial state\n",
        "\n",
        "while state != num_states - 1:\n",
        "    action = np.argmax(Q[state])  # Choose action with highest Q-value\n",
        "\n",
        "    print(\"Current state:\", state)\n",
        "    print(\"Action:\", action)\n",
        "\n",
        "    state = np.random.randint(0, num_states)  # Randomly select next state\n",
        "\n",
        "print(\"Reached the goal stat!e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcrLa9KOf1aT",
        "outputId": "6a15a281-e6b1-4ecb-e1bb-c248078257f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state: 0\n",
            "Action: 0\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Current state: 3\n",
            "Action: 1\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Current state: 1\n",
            "Action: 0\n",
            "Current state: 0\n",
            "Action: 0\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Reached the goal stat!e!\n"
          ]
        }
      ]
    }
  ]
}