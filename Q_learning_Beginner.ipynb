{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Il3waFud1ABY"
      ],
      "authorship_tag": "ABX9TyMiH8JrCMS1j0uNPMnZB191",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZAHID188/Q-learning_collab/blob/main/Q_learning_Beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning lesson\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFIBR75pfCdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Q-table: The Q-table is a matrix that represents the Q-values for each state-action pair. In this example, we initialize a Q-table with zeros."
      ],
      "metadata": {
        "id": "o3cTq3-MgE-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the Q-table\n",
        "num_states = 5\n",
        "num_actions = 3\n",
        "Q = np.zeros((num_states, num_actions)) #make a row-5, column-3 array   Creating a 2-dimensional array\n",
        "print(Q)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwYsZieqfO-u",
        "outputId": "74dd91b9-8a2e-411b-e7f8-3472724b5c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the rewards matrix: The rewards matrix defines the rewards for each state-action pair. In this example, we use a 5x3 matrix where -1 represents a negative reward and 100 represents a positive reward for reaching the goal state.\n",
        "\n",
        "The value 0 in the last row indicates that no reward is given for taking an action in the goal state."
      ],
      "metadata": {
        "id": "KLYgdtm8hI6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the rewards matrix\n",
        "rewards = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [-1, -1, -1],\n",
        "    [0, -1, 100]])\n",
        "print(rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHY-i9z7ffln",
        "outputId": "8661a9ea-6448-4529-f421-45f49958cb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [ -1  -1  -1]\n",
            " [  0  -1 100]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the discount factor: The discount factor (gamma) determines the importance of future rewards compared to immediate rewards. In this example, we set gamma to 0.8.\n",
        "\n",
        "Define the learning rate: The learning rate determines the impact of new information on the Q-values. In this example, we set the learning rate to 0.5.\n",
        "\n",
        "\n",
        "Define the number of episodes: An episode represents a complete run-through of the Q-learning algorithm. In this example, we set the number of episodes to 1000."
      ],
      "metadata": {
        "id": "NJfEtx49hhhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discount factor\n",
        "gamma = 0.8\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Define the number of episodes\n",
        "num_episodes = 1000\n",
        "\n"
      ],
      "metadata": {
        "id": "IN1SSwc6fs7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Q-learning algorithm: The Q-learning algorithm consists of multiple episodes, where the agent learns to update the Q-values based on the rewards received.\n",
        "    \n",
        "    a. Initialize the state: At the start of each episode, we randomly select an initial state.\n",
        "    \n",
        "    ```python\n",
        "    state = np.random.randint(0, num_states)\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    b. Perform Q-learning until the goal state is reached:\n",
        "    \n",
        "    i. Choose an action: The agent selects an action based on the highest Q-value for the current state.\n",
        "    \n",
        "    ```python\n",
        "    action = np.argmax(Q[state])\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    ii. Transition to the next state: The agent transitions to the next state by randomly selecting one.\n",
        "    \n",
        "    ```python\n",
        "    next_state = np.random.randint(0, num_states)\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    iii. Get the reward: The agent receives a reward based on the current state-action pair.\n",
        "    \n",
        "    ```python\n",
        "    reward = rewards[state, action]\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    iv. Update the Q-value: The Q-value for the current state-action pair is updated using the Q-learning equation.\n",
        "    \n",
        "    ```python\n",
        "    Q[state, action] += learning_rate * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    v. Move to the next state: The agent moves to the next state.\n",
        "    \n",
        "    ```python\n",
        "    state = next_state\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    vi. Check if the goal state is reached: If the agent reaches the goal state, the episode ends.\n",
        "    \n",
        "    ```python\n",
        "    if state == num_states - 1:\n",
        "        done = True\n",
        "    \n",
        "    ```"
      ],
      "metadata": {
        "id": "-3zEqpGSiHJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## np.argmax\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "# Create a 1-dimensional array\n",
        "arr = np.array([2, 5, 1, 8, 4])\n",
        "\n",
        "# Find the index of the maximum value in the array\n",
        "max_index = np.argmax(arr)\n",
        "\n",
        "print(max_index)\n",
        "# Output: 3\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OQBCPG1RjC8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning algorithm"
      ],
      "metadata": {
        "id": "uM566aosjeKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(0, num_states)  # Initial state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(Q[state])  # Choose action with highest Q-value\n",
        "\n",
        "        next_state = np.random.randint(0, num_states)  # Randomly select next state\n",
        "        reward = rewards[state, action]  # Get reward for current state-action pair\n",
        "\n",
        "        Q[state, action] += learning_rate * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "        if state == num_states - 1:\n",
        "            done = True\n",
        "\n"
      ],
      "metadata": {
        "id": "pCI5hEgvfzE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the learned policy\n",
        "state = 0  # Initial state\n",
        "\n",
        "while state != num_states - 1:\n",
        "    action = np.argmax(Q[state])  # Choose action with highest Q-value\n",
        "\n",
        "    print(\"Current state:\", state)\n",
        "    print(\"Action:\", action)\n",
        "\n",
        "    state = np.random.randint(0, num_states)  # Randomly select next state\n",
        "\n",
        "print(\"Reached the goal stat!e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcrLa9KOf1aT",
        "outputId": "6a15a281-e6b1-4ecb-e1bb-c248078257f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state: 0\n",
            "Action: 0\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Current state: 3\n",
            "Action: 1\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Current state: 1\n",
            "Action: 0\n",
            "Current state: 0\n",
            "Action: 0\n",
            "Current state: 2\n",
            "Action: 0\n",
            "Reached the goal stat!e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#snake game"
      ],
      "metadata": {
        "id": "QbjIJnYgHTmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "WINDOW_WIDTH = 600\n",
        "WINDOW_HEIGHT = 600\n",
        "BLOCK_SIZE = 20\n",
        "GAME_SPEED = 100  # Higher value means slower game\n",
        "\n",
        "# Colors\n",
        "BLACK = (0, 0, 0)\n",
        "WHITE = (255, 255, 255)\n",
        "RED = (255, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "\n",
        "# Q-learning parameters\n",
        "EPSILON = 0.9  # Exploration rate\n",
        "ALPHA = 0.1  # Learning rate\n",
        "GAMMA = 0.6  # Discount factor\n",
        "EPISODES = 25000  # Number of games to train\n",
        "\n",
        "# Initialize PyGame\n",
        "pygame.init()\n",
        "font = pygame.font.Font(None, 25)\n",
        "clock = pygame.time.Clock()\n",
        "\n",
        "# Q-table\n",
        "q_table = {}\n",
        "\n",
        "# Helper functions\n",
        "def get_state(head, food):\n",
        "    \"\"\"\n",
        "    Get the current state of the game.\n",
        "    \"\"\"\n",
        "    x1, y1 = head.x, head.y\n",
        "    x2, y2 = food.x, food.y\n",
        "    return (x1 - x2, y1 - y2)\n",
        "\n",
        "def get_next_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    Get the next action to take based on the current state and epsilon value.\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, 3)  # Explore\n",
        "    else:\n",
        "        values = q_table.get(state, [0, 0, 0, 0])\n",
        "        return np.argmax(values)  # Exploit\n",
        "\n",
        "def update_q_table(state, action, reward, next_state):\n",
        "    \"\"\"\n",
        "    Update the Q-table based on the current state, action, reward, and next state.\n",
        "    \"\"\"\n",
        "    q_value = q_table.get(state, [0, 0, 0, 0])\n",
        "    next_q_value = q_table.get(next_state, [0, 0, 0, 0])\n",
        "    max_next_q_value = max(next_q_value)\n",
        "    q_value[action] += ALPHA * (reward + GAMMA * max_next_q_value - q_value[action])\n",
        "    q_table[state] = q_value\n",
        "\n",
        "# Snake class\n",
        "class Snake:\n",
        "    def __init__(self):\n",
        "        self.x = WINDOW_WIDTH // 2\n",
        "        self.y = WINDOW_HEIGHT // 2\n",
        "        self.body = [(self.x, self.y)]\n",
        "        self.direction = 0  # 0: up, 1: right, 2: down, 3: left\n",
        "\n",
        "    def move(self, action):\n",
        "        \"\"\"\n",
        "        Move the snake based on the given action.\n",
        "        \"\"\"\n",
        "        x = self.body[0][0]\n",
        "        y = self.body[0][1]\n",
        "        if action == 0:\n",
        "            y -= BLOCK_SIZE  # Up\n",
        "        elif action == 1:\n",
        "            x += BLOCK_SIZE  # Right\n",
        "        elif action == 2:\n",
        "            y += BLOCK_SIZE  # Down\n",
        "        else:\n",
        "            x -= BLOCK_SIZE  # Left\n",
        "\n",
        "        self.body.insert(0, (x, y))\n",
        "        self.body.pop()\n",
        "\n",
        "    def draw(self, surface):\n",
        "        \"\"\"\n",
        "        Draw the snake on the game surface.\n",
        "        \"\"\"\n",
        "        for x, y in self.body:\n",
        "            pygame.draw.rect(surface, GREEN, (x, y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "\n",
        "# Food class\n",
        "class Food:\n",
        "    def __init__(self):\n",
        "        self.x = random.randint(0, WINDOW_WIDTH - BLOCK_SIZE) // BLOCK_SIZE * BLOCK_SIZE\n",
        "        self.y = random.randint(0, WINDOW_HEIGHT - BLOCK_SIZE) // BLOCK_SIZE * BLOCK_SIZE\n",
        "\n",
        "    def draw(self, surface):\n",
        "        \"\"\"\n",
        "        Draw the food on the game surface.\n",
        "        \"\"\"\n",
        "        pygame.draw.rect(surface, RED, (self.x, self.y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "\n",
        "# Game loop\n",
        "def play_game(display):\n",
        "    snake = Snake()\n",
        "    food = Food()\n",
        "    score = 0\n",
        "    game_over = False\n",
        "\n",
        "    while not game_over:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                game_over = True\n",
        "\n",
        "        state = get_state(snake.body[0], food)\n",
        "        action = get_next_action(state, EPSILON)\n",
        "        snake.move(action)\n",
        "\n",
        "        # Check if the snake has eaten the food\n",
        "        if snake.body[0][0] == food.x and snake.body[0][1] == food.y:\n",
        "            score += 1\n",
        "            food = Food()\n",
        "            snake.body.append(snake.body[-1])\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        # Check if the snake has collided with itself or the boundary\n",
        "        x, y = snake.body[0]\n",
        "        if x < 0 or x >= WINDOW_WIDTH or y < 0 or y >= WINDOW_HEIGHT or (x, y) in snake.body[1:]:\n",
        "            game_over = True\n",
        "            reward = -1\n",
        "\n",
        "        next_state = get_state(snake.body[0], food)\n",
        "        update_q_table(state, action, reward, next_state)\n",
        "\n",
        "        display.fill(BLACK)\n",
        "        snake.draw(display)\n",
        "        food.draw(display)\n",
        "        score_text = font.render(f\"Score: {score}\", True, WHITE)\n",
        "        display.blit(score_text, (10, 10))\n",
        "        pygame.display.update()\n",
        "        clock.tick(GAME_SPEED)\n",
        "\n",
        "    return score\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    display = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
        "    pygame.display.set_caption(\"Snake Game\")\n",
        "\n",
        "    max_score = 0\n",
        "    for episode in range(EPISODES):\n",
        "        score = play_game(display)\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "        print(f\"Episode: {episode + 1}, Score: {score}, Max Score: {max_score}\")\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "# Start training\n",
        "train()"
      ],
      "metadata": {
        "id": "91xg5vYmHV26",
        "outputId": "7d97a4c8-9b44-4354-ee52-6d01f351c202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'x'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a4ac4fa8e701>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-a4ac4fa8e701>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a4ac4fa8e701>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(display)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a4ac4fa8e701>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(head, food)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mGet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'x'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how the code works:\n",
        "\n",
        "*   The Snake class represents the snake in the game. It has a body (a list of coordinates), a direction, and methods to move and draw the snake.\n",
        "*   The Food class represents the food in the game. It has random coordinates and a method to draw the food.\n",
        "The play_game function is the main game loop. It initializes the snake, food, and score, and then iterates through the game loop until the game is over.\n",
        "*   In each iteration of the game loop, the current state is obtained using the get_state function, which calculates the relative position of the snake's head and the food.\n",
        "*   The next action is determined using the get_next_action function, which either explores (random action) or exploits (action with the highest Q-value) based on the epsilon value.\n",
        "*   The snake moves according to the chosen action.\n",
        "*   If the snake eats the food, the score is incremented, a new food is generated, and the snake's body grows. A positive reward is given.\n",
        "*   If the snake collides with itself or the boundary, the game is over, and a negative reward is given.\n",
        "*   The Q-table is updated using the update_q_table function, which calculates the new Q-value based on the current state, action, reward, and next state.\n",
        "*   The game is rendered using PyGame, and the score is displayed.\n",
        "*   The train function is the main entry point. It creates a PyGame window, runs the training loop for a specified number of episodes, and keeps track of the maximum score achieved.\n",
        "\n",
        "During training, the snake learns to play the game by updating the Q-table based on the rewards it receives. After training, the snake should be able to play the game effectively by exploiting the learned Q-values.\n",
        "\n",
        "Note that this implementation doesn't include any visualization of the training process or the ability to play the game after training. You can modify the code to add these features if desired."
      ],
      "metadata": {
        "id": "Vpt4d_mCHgVQ"
      }
    }
  ]
}